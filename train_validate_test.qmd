---
title: "Model Selection with Train/Validate/Test"
format: html
editor: visual
---

# Assignment 3: Model Selection with Train/Validate/Test

You’ll turn this `.qmd` file in as your final modeling assignment. Save, commit, and push to GitHub. Then, go to Canvas and type "Submitted" under the assignment submission. Assignment due Sunday 4/6 at 11:59 pm.

------------------------------------------------------------------------

In this activity, you'll practice selecting the best predictive model using a **train/validate/test split**. This is one step beyond a train/test split.

You’ll compare multiple models using both *in-sample evaluation* (like AIC and ANOVA) and *out-of-sample validation* (using RMSE). You'll then evaluate your final model on a held-out test set.

## Why Use Train/Validate/Test Instead of Just Train/Test?

In a basic train/test split:

-   You train your model on one part of the data

-   You test its performance on the rest

But what if you want to compare multiple models?

If you use the test set to pick the best one, you've “peeked” — and the test set is no longer a fair estimate of how your model performs on truly unseen data.

So we add a **validation set**:

-   **Training set** → Fit multiple models

-   **Validation set** → Choose the best model

-   **Test set** → Evaluate final model performance

This approach helps prevent overfitting and gives you a more realistic estimate of how your model will perform in the real world.

## Set Up Packages

Add packages as needed.

```{r}
# Setup
library(tidyverse)
library(caret)
library(Metrics)
set.seed(42)  # for reproducibility
```

## Dataset Requirements

You may choose your own dataset for this assignment.

Choose a dataset that:

-   Has a numeric outcome variable you want to predict

-   Contains at least 3-4 predictors (numeric or categorical)

-   Is either:

    -   A built-in dataset in R (e.g., `diamonds`, `Boston`, `iris`, `mtcars`, `airquality`, `penguins`, etc.)
    -   From your final project
    -   Any other dataset we've used in class

> If you're not sure what dataset to use, try `Boston`:

```{r}
library(MASS)
data <- mtcars
```

## Step 1: Split the Data

Split the data into: 60% training, 20% validation, and 20% test

```{r}
# edit below as needed
train_index <- createDataPartition(data$mpg, p = 0.6, list = FALSE)
train_data <- data[train_index, ] # training data
temp_data <- data[-train_index, ]

val_index <- createDataPartition(temp_data$mpg, p = 0.5, list = FALSE)
val_data <- temp_data[val_index, ] # validation data
test_data <- temp_data[-val_index, ] # test data
```

## Step 2: Fit Multiple Models

Create at least three models of increasing complexity:

```{r}
# edit below as needed
model_1 <- lm(mpg ~ cyl, data = train_data)
model_2 <- lm(mpg ~ cyl + disp + hp, data = train_data)
model_3 <- lm(mpg ~ cyl * disp + hp + wt, data = train_data)

summary(model_1)
summary(model_2)
summary(model_3)
```

**Questions:**

-   Which model seems to be the best fit according to the Adjusted R^2^ value?

The best model according to the Adjusted R^2^ value is model 3, having a 0.8543 R^2^ value

> You may look at R² and Adjusted R² on the training set to help understand model fit, but to ultimately choose the best model, you'll use RMSE on the validation set below.\
> RMSE gives you a more honest view of how well your model predicts on new data.

## Step 3: Compare Using AIC and ANOVA

> -   AIC helps you compare model fit while penalizing complexity
>
> -   ANOVA tests whether adding predictors significantly improves the model

```{r}
# AIC
AIC(model_1)
AIC(model_2)
AIC(model_3)

# ANOVA for nested comparisons
anova(model_1, model_2)
anova(model_2, model_3)
```

**Questions:**

-   Which model has the lowest AIC?

    -   Model 3 has the lowest AIC, scoring a 93.85387

-   Are the improvements in fit (from the `anova` output) statistically significant?

    -   The improvements in fit between models 2 and 3 are statically significant according to the p-value in the anova test

## Step 4: Evaluate on the Validation Set (RMSE) (new)

The validation set allows us to compare models fairly and reevaluate our choices before making a final decision. If a model performs well on training but poorly on validation, we might consider simplifying or adjusting the model before moving on to the test set.

```{r}
# edit below as needed
rmse(val_data$mpg, predict(model_1, val_data))
rmse(val_data$mpg, predict(model_2, val_data))
rmse(val_data$mpg, predict(model_3, val_data))
```

**Questions:**

-   Which model performed best on the validation set?

    -   Model 3 performed the best on the validation set because it had the lowest value compared to the other models

-   Does that match what AIC/ANOVA suggested?

    -   Yes, it does match the AIC and anova tests, so it seems like model 3 is the best overall model out of the three

## Step 5: Choose the Best Model

Pick the model with the best validation RMSE. Assign it to a variable called `final_model`. This isn't a "required" step, but it keeps things neat when you only need to define the final model in one spot.

```{r}
final_model <- model_3
```

## Step 6: Test the Final Model

Now evaluate your chosen model on the test set:

```{r}
# edit below as needed
rmse(test_data$mpg, predict(final_model, test_data))
```

**Questions:**

-   Is the test RMSE close to the validation RMSE?

    -   In my opinion I believe the test RMSE and the validation are pretty close and only around 1 mpg apart from each other. When thinking about it, only being off \~1 mpg compared to 3, 4, or 5 is pretty close to being accurate.

-   What does that say about how well the model generalizes?

    -   Based on the validity test and AIC and anova models we ran, I do believe model 3 may statistically be the best fit model out of the three. However, when looking back on our first comparison between models 1 and 2 and not finding any significant difference between the two may mean that the other predictors such as disp and hp may point that a simpler model like model 1 may be better generalizing the data. This could also point that model 3 could be a product of over fitting but I think we would need more data and information to find out.

## Step 7: Compare All RMSE Values

```{r}
# edit below as needed
rmse(train_data$mpg, predict(final_model, train_data)) # training set
rmse(val_data$mpg, predict(final_model, val_data)) # validation set
rmse(test_data$mpg, predict(final_model, test_data)) # test set
```

**Questions:**

-   Is there a big gap between training and validation/test RMSE? If so, does that suggest over fitting?
    -   Yes, there is a gap between the training RMSE (1.62) and the validation RMSE (3.75), which suggests potential over fitting. So my hypothesis of potential over fitting could be true.

## Summary Questions

Answer the following. Use full sentences.

1.  Which model did you choose, and why?

    I chose Model 3 as the best fitting model because it had the lowest AIC value, highest adjusted R^2^ value and best validation between the other two models. The anova test also indicated that adding predictors like disp, hp, and wt, improved the model significantly. While there is some concern about over fitting, Model 3 seemed the best fitting among the models tested.

2.  What were the AIC values for each model?

    Model 1: 104.6066

    Model 2: 107.5582

    Model 3: 93.85387

3.  Did ANOVA support the improvements?

    Yes, the ANOVA results supported model 3 being the most fit. While there was no statistically significant difference between Model 1 and Model 2, Model 3 showed a significant improvement over Model 2, indicating that the additional interaction term and predictors contributed to the model’s fitness over the other two.

4.  What were the RMSE values for training, validation, and test sets?

    Training: 1.619867

    Validation: 3.749996

    Test set: 2.408877

5.  How confident are you that your model will generalize well?

    I am confident that the model will generalize well, as long as we can prove there is no over fitting. The gap between the training and validation RMSE suggests some over fitting. A simpler model, such as Model 1, might generalize better despite having slightly worse in-sample metrics. However, based on the other tests such as the R^2^ , AIC, ANOVA I feel confident that model 3 is the best fitting model and should be able to generalize well.

    *Reminder: Your chosen model should balance good in-sample fit (R², AIC) with strong out-of-sample performance (validation RMSE), and generalize well to the test set. You don’t have to pick the “most complex” model — just the one that performs reliably and addresses the research question.*
